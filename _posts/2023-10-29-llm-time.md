---
title: 'Large Language Models Are Zero-Shot Time Series Forecasters'
date: 2023-10-29
permalink: /posts/2023/10/llm-time/
tags:
  - ai
  - finance
  - timeseries forecasting
---

[Paper](https://arxiv.org/abs/2310.07820) | 
[Code](https://github.com/ngruver/llmtime)
<img src='/images/llmtime_top_fig.png' style='display:block; margin:auto;'>

Abstract
======
- By encoding time series as a string of numerical digits, we can frame time series forecasting as next-token prediction in text.
- How to effectively tokenize time series data?
  - proper tokenization is extremely important
  - the most common tokenization method is byte-pair encoding (BPE), which treats inputs as bit strings and assign tokens based on the rate of ocurrence in the training corpus, optimizing for shorter sequeces of tokens on average.
- Convert discrete distributions over tokens into highly flexible densities over continuous values
- The success of LLms for time series could come from their abilitiy to naturally represent multimodal distributions, in conjuction with biases for simplicity and repetition, which capture salient features in many time series, such as seasonal trends.
- LLMs can also naturally handle missing data without imputation through non-numerical text
- Alignment intervention (such as RLHF) might worsen performance


Introduction
======
- Unlike video and audio, which typically have consistent input scales and sampling rates, aggregated time series datasets often compirse sequences from radically different sources, sometimes with missing values
- Uncertainty estimation in Financial data is specially important to extrapolate from observation containing a tiny fraction of possible information.
- There is still no consensus on an unsupervised objective for large scale pretraining for time series modeling, neither large, cohesive datasets.
- Consequently, simple time series methods (e.g. ARIMA, and linear models) often outperform deep learning methods on popular benchmarks.
- LLMTime proposes at its core to represent the time series as a string of numerical digits, and views time series forecasting as next-token prediction in text, unlocking the use of powerful pretrained models and probabilistic capacities, such as likelihood evaluation and sampling.


Background
======
- Proper tokenization is extremely important, and small details can have surprisingly significant effects
- Touvron et al. designed the LLaMA tokenizer to map numbers to individual digits, which lead to significant improvements in mathematical abilities, with small LLaMA models outperforming GPT-4
- Evaluation: [CRPS](https://www.lokad.com/continuous-ranked-probability-score) (Continuous Ranked Probability Score)
- Zhou et al. FPT, which finetunes a BERT encoder to perform time series forecasting
- Zhang et al. intoduce Meta-Transformer
- PromptCat poses forecastingas a question answering with prompting

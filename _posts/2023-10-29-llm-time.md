---
title: 'Large Language Models Are Zero-Shot Time Series Forecasters'
date: 2023-10-29
permalink: /posts/2023/10/llm-time/
tags:
  - ai
  - finance
  - timeseries forecasting
---

[Paper](https://arxiv.org/abs/2310.07820) | 
[Code](https://github.com/ngruver/llmtime)
<img src='/images/posts/2023-10-llm-time/fig1.png' style='display:block; margin:auto;'>


Abstract
======
- By encoding time series as a string of numerical digits, we can frame time series forecasting as next-token prediction in text.
- This work shows that the zero-shot generalization abilities of LLMs and their preference for compressible patterns extend well beyond language understanding and can be used for time series forecasting.
- How to effectively tokenize time series data?
  - proper tokenization is extremely important
  - the most common tokenization method is byte-pair encoding (BPE), which treats inputs as bit strings and assign tokens based on the rate of ocurrence in the training corpus, optimizing for shorter sequeces of tokens on average.
- Convert discrete distributions over tokens into highly flexible densities over continuous values
- The success of LLms for time series could come from their abilitiy to naturally represent multimodal distributions, in conjuction with biases for simplicity and repetition, which capture salient features in many time series, such as seasonal trends.
- LLMs can also naturally handle missing data without imputation through non-numerical text
- Alignment intervention (such as RLHF) might worsen performance


Introduction
======
- Unlike video and audio, which typically have consistent input scales and sampling rates, aggregated time series datasets often compirse sequences from radically different sources, sometimes with missing values
- Uncertainty estimation in Financial data is specially important to extrapolate from observation containing a tiny fraction of possible information.
- There is still no consensus on an unsupervised objective for large scale pretraining for time series modeling, neither large, cohesive datasets.
- Consequently, simple time series methods (e.g. ARIMA, and linear models) often outperform deep learning methods on popular benchmarks.
- LLMTime proposes at its core to represent the time series as a string of numerical digits, and views time series forecasting as next-token prediction in text, unlocking the use of powerful pretrained models and probabilistic capacities, such as likelihood evaluation and sampling.


Background
======
- Proper tokenization is extremely important, and small details can have surprisingly significant effects
- LLMs are few-shot learners with incresed parameter count and training data, with *zero-shot generalization* though *in-context learning*, identifying patterns in prompts and extrapolating though the next-token prediction.
- In-context learning might emerge from extensive compression of the input data
- Compression favors learning algorithms that operate over input data with programmatic abstractions, for example *context-free* grammars and *induction heads*, which can implement copy-and-paste type operations for generating samples with highly structured syntax.
- BPE compresses numbers based on frequency of occurrence in the training data, so numbers can be broken down into awkward chunks that make learning basic numerical operations challenging
- Touvron et al. designed the LLaMA tokenizer to map numbers to individual digits, which lead to significant improvements in mathematical abilities, with small LLaMA models outperforming GPT-4
- The other challenge of applying language models to time series data is proper evaluation.
  - [CRPS](https://www.lokad.com/continuous-ranked-probability-score) (Continuous Ranked Probability Score)
- LLMs can assign likelihoods to full sequences of time series data, and we show how a small modification to an LLM’s discrete likelihood can yield a continuous density that is useful for model comparison

- FPT finetunes a BERT encoder to perform time series forecasting
- Meta-Transformer: framework for finetuning a LM for non-text modalities, including time series.
- PromptCat proposes forecasting as a question answering with prompting

<img src='/images/posts/2023-10-llm-time/fig2.png' style='display:block; margin:auto;'>


LLMTime
======
- Common tokenization methods like BPE tend to break a single number into tokens that don’t align with the digits, which can make arithmetic considerably more difficult
- Because decimal points are redundant given a fixed precision, we drop them in the encoding to save on context length
$0.123, 1.23, 12.3, 123.0 → " 1 2 , 1 2 3 , 1 2 3 0 , 1 2 3 0 0"$

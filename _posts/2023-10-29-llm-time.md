---
title: 'Large Language Models Are Zero-Shot Time Series Forecasters'
date: 2023-10-29
permalink: /posts/2023/10/llm-time/
tags:
  - ai
  - finance
  - timeseries forecasting
---

Papers
======
[Large Language Models Are Zero-Shot Time Series Forecasters](https://arxiv.org/abs/2310.07820)
<img src='/images/llmtime_top_fig.png' style='display:block; margin:auto;'>
[Code](https://github.com/ngruver/llmtime)

- How to effectively tokenize time series data
  - proper tokenization is extremely important
  - the most common tokenization method is byte-pair encoding (BPE), which treats inputs as bit strings and assign tokens based on the rate of ocurrence in the training corpus, optimizing for shorter sequeces of tokens on average.
- Convert discrete distributions over tokens into highly flexible densities over continuous values
- Handle missing data without imputation through non-numerical text
- Alignment intervention (RLHF) might worsen performance
- Touvron et al. designed the LLaMA tokenizer to map numbers to individual digits, which lead to significant improvements in mathematical abilities, with small LLaMA models outperforming GPT-4
- Evaluation: [CRPS](https://www.lokad.com/continuous-ranked-probability-score) (Continuous Ranked Probability Score)
- Zhou et al. FPT, which finetunes a BERT encoder to perform time series forecasting
- Zhang et al. intoduce Meta-Transformer
- PromptCat poses forecastingas a question answering with prompting
